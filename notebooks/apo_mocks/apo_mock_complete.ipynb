{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "#\n",
    "# TITLE - apo_mock_complete\n",
    "# AUTHOR - James Lane\n",
    "# PROJECT - ges-mass\n",
    "#\n",
    "# ------------------------------------------------------------------------\n",
    "#\n",
    "# Docstrings and metadata:\n",
    "'''Use all the functions to generate complete sets of samples\n",
    "'''\n",
    "\n",
    "__author__ = \"James Lane\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "\n",
    "## Basic\n",
    "import numpy as np\n",
    "import sys, os, pdb, copy, glob, subprocess, warnings, dill as pickle, time, gc\n",
    "from astropy import units as apu\n",
    "\n",
    "## Matplotlib\n",
    "# import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "## galpy\n",
    "from galpy import orbit\n",
    "from galpy import potential\n",
    "# from galpy import actionAngle as aA\n",
    "from galpy import df\n",
    "from galpy.util import _rotate_to_arbitrary_vector\n",
    "\n",
    "## APOGEE, isochrones, dustmaps\n",
    "from apogee import select as apsel\n",
    "from isodist import Z2FEH,FEH2Z\n",
    "import mwdust\n",
    "from mwdust.util.extCurves import aebv\n",
    "\n",
    "## scipy\n",
    "import scipy.integrate\n",
    "import scipy.interpolate\n",
    "\n",
    "## Astropy and healpix\n",
    "from astropy.coordinates import SkyCoord\n",
    "import healpy\n",
    "\n",
    "## APOGEE mocks\n",
    "import apomock\n",
    "\n",
    "### Scale parameters\n",
    "ro = 8.275\n",
    "vo = 220\n",
    "zo = 0.0208 # Bennett+ 2019\n",
    "\n",
    "### Notebook setup\n",
    "%matplotlib inline\n",
    "plt.style.use('../../../src/mpl/project.mplstyle') # This must be exactly here\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def chabrier_imf(m,k=0.0193,A=1.):\n",
    "    '''chabrier_imf:\n",
    "    Chabrier initial mass function\n",
    "    '''\n",
    "    k = 0.0193 # Equalizes m<1 and m>1 at m=1\n",
    "    a = 2.3\n",
    "    \n",
    "    if not isinstance(m,np.ndarray):\n",
    "        m = np.atleast_1d(m)\n",
    "    ##fi\n",
    "    \n",
    "    where_m_gt_1 = m>1\n",
    "    Nm = np.empty(len(m))\n",
    "    Nm[~where_m_gt_1] = (0.158/(np.log(10)*m[~where_m_gt_1]))\\\n",
    "                        *np.exp(-(np.log10(m[~where_m_gt_1])-np.log10(0.08))**2\\\n",
    "                               /(2*0.69**2))\n",
    "    Nm[where_m_gt_1] = k*m[where_m_gt_1]**(-a)\n",
    "    Nm[m<0.01] = 0\n",
    "    return A*Nm\n",
    "#def\n",
    "\n",
    "def kroupa_imf(m,k1=1.):\n",
    "    '''kroupa_imf:\n",
    "    Kroupa initial mass function\n",
    "    '''\n",
    "    a1,a2,a3 = 0.3,1.3,2.3\n",
    "    k2 = 0.08*k1\n",
    "    k3 = 0.5*k2\n",
    "    \n",
    "    if not isinstance(m,np.ndarray):\n",
    "        m = np.atleast_1d(m)\n",
    "    ##fi\n",
    "    \n",
    "    where_m_1 = np.logical_and(m>=0.01,m<0.08)\n",
    "    where_m_2 = np.logical_and(m>=0.08,m<0.5)\n",
    "    where_m_3 = m>=0.5\n",
    "    Nm = np.empty(len(m))\n",
    "    Nm[where_m_1] = k1*m[where_m_1]**(-a1)\n",
    "    Nm[where_m_2] = k2*m[where_m_2]**(-a2)\n",
    "    Nm[where_m_3] = k3*m[where_m_3]**(-a3)\n",
    "    Nm[m<0.01] = 0\n",
    "    return Nm\n",
    "#defs\n",
    "\n",
    "def _remove_orbits_outside_footprint(orbs,aposf,field_indx=None,\n",
    "                                     chunk_size=None):\n",
    "    '''_remove_orbits_outside_footprint:\n",
    "    \n",
    "    Remove stellar samples from outside the APOGEE observational footprint.\n",
    "    Each plate has a variable field of view, and an inner 'hole' of \n",
    "    5 arcminutes. Using field_indx allows for selecting only a subset of the \n",
    "    available fields to use.\n",
    "    \n",
    "    Args:\n",
    "        orbs (array) - Orbits representing samples\n",
    "        aposf (array) - APOGEE selection function\n",
    "        field_indx (array) - Indices of fields to consider\n",
    "        chunk_size (int) - If not None, split up orbs into chunks of this \n",
    "            size to \n",
    "    \n",
    "    Returns:\n",
    "        fp_indx () - Index of samples that lie within the observational\n",
    "            footprint\n",
    "        fp_locid () - Location IDs of field each sample lies within\n",
    "    '''\n",
    "    # Account for field_indx, fields we want to consider\n",
    "    if field_indx is None:\n",
    "        field_indx = np.arange(0,len(aposf._apogeeFields),dtype=int)\n",
    "    ##fi\n",
    "    \n",
    "    # field center coordinates, location IDs, radii\n",
    "    glon = aposf._apogeeField['GLON'][field_indx]\n",
    "    glat = aposf._apogeeField['GLAT'][field_indx]\n",
    "    locids = aposf._locations[field_indx]\n",
    "    radii = np.zeros(len(field_indx))\n",
    "    for i in range(len(locids)):\n",
    "        radii[i] = aposf.radius(locids[i])\n",
    "    ###i\n",
    "    \n",
    "    # Make SkyCoord objects\n",
    "    aposf_sc = SkyCoord(frame='galactic', l=glon*apu.deg, b=glat*apu.deg)\n",
    "    orbs_sc = SkyCoord(frame='galactic', l=orbs.ll(), b=orbs.bb())\n",
    "    \n",
    "    # First nearest-neighbor match\n",
    "    indx,sep,_ = orbs_sc.match_to_catalog_sky(aposf_sc)\n",
    "    indx_radii = radii[indx]\n",
    "    indx_locid = locids[indx]\n",
    "    fp_indx = np.where(np.logical_and(sep < indx_radii*apu.deg,\n",
    "                                      sep > 5.5*apu.arcmin))[0]\n",
    "    fp_locid = indx_locid[fp_indx]\n",
    "    \n",
    "    # Second nearest-neighbor match for samples inside plate central holes\n",
    "    where_in_hole = np.where(sep < 5.5*apu.arcmin)[0]\n",
    "    indx2,sep2,_ = orbs_sc[where_in_hole].match_to_catalog_sky(aposf_sc,\n",
    "                                                               nthneighbor=2)\n",
    "    indx2_radii = radii[indx2]\n",
    "    indx2_locid = locids[indx2]\n",
    "    fp_indx2 = np.where(np.logical_and(sep2 < indx2_radii*apu.deg,\n",
    "                                       sep2 > 5.5*apu.arcmin))[0]\n",
    "    if len(fp_indx2) > 0:\n",
    "        fp_indx = np.append(fp_indx,where_in_hole[fp_indx2])\n",
    "        fp_locid = np.append(fp_locid,indx2_locid[fp_indx2])\n",
    "    ##fi\n",
    "    \n",
    "    return fp_indx,fp_locid\n",
    "#def\n",
    "\n",
    "def _match_isochrone_to_samples(iso,ms,m_err=0.05,iso_keys=_parsec_1_2_iso_keys):\n",
    "    '''_match_isochrone_to_samples:\n",
    "    \n",
    "    Match the samples to entries in an isochrone according to initial mass\n",
    "    \n",
    "    iso_keys must accept the following keys:\n",
    "    'Mini' -> initial mass key\n",
    "    \n",
    "    Args:\n",
    "        iso (array) - isochrone array\n",
    "        ms (array) - sample masses\n",
    "        m_err (float) - Maximum difference in mass between sample and isochrone\n",
    "            for successful match\n",
    "        iso_keys (dict) - Dictionary of keys for accessing the isochrone \n",
    "            properties, accessible via a common set of strings (see above)\n",
    "        \n",
    "    Returns:\n",
    "        good_match (array) - Indices of ms which found matches in the isochrone \n",
    "            array within m_err tolerance\n",
    "        match_indx (array) - array of matches, length len(good_match), \n",
    "            indexing ms into iso\n",
    "    '''\n",
    "    # Access initial mass\n",
    "    mass_initial_key = iso_keys['mass_initial']\n",
    "    m0 = iso[mass_initial_key]\n",
    "    \n",
    "    # Ensure isochrone is sorted by initial mass\n",
    "    m0_argsort = np.argsort(m0)\n",
    "    m0_sorted = m0[m0_argsort]\n",
    "    \n",
    "    # Search the sorted array for the nearest neighbors (fast)\n",
    "    m0_mids = m0_sorted[1:] - np.diff(m0_sorted.astype('f'))/2\n",
    "    idx = np.searchsorted(m0_mids, ms)\n",
    "    cand_indx = m0_argsort[idx]\n",
    "    residual = ms - m0_sorted[cand_indx]\n",
    "    \n",
    "    good_match = np.where( (np.abs(residual) < m_err) &\\\n",
    "                           (ms < m0[-1]) &\\\n",
    "                           (ms > m0[0])\n",
    "                          )[0]\n",
    "\n",
    "    match_indx = np.argsort(m0_argsort)[cand_indx[good_match]]\n",
    "\n",
    "    np.all(np.abs(ms[good_match]-m0[match_indx]) <= m_err)\n",
    "    \n",
    "    return good_match,match_indx\n",
    "#def\n",
    "\n",
    "def _sample_r(denspot,n=1,r_min=0.,r_max=np.inf,a=1.):\n",
    "    '''_sample_r:\n",
    "    \n",
    "    Draw radial position samples. Note the function interpolates the normalized \n",
    "    iCMF onto the variable xi, defined as:\n",
    "    \n",
    "    .. math:: \\\\xi = \\\\frac{r/a-1}{r/a+1}\n",
    "    \n",
    "    so that xi is in the range [-1,1], which corresponds to an r range of \n",
    "    [0,infinity)\n",
    "    \n",
    "    Args:\n",
    "        denspot (galpy.potential.Potential) - galpy potential representing\n",
    "            the density profile. Must be spherical\n",
    "        n (int) - Number of samples\n",
    "        r_min (float) - \n",
    "        r_max (float) - \n",
    "        a (float) - \n",
    "        \n",
    "    Returns:\n",
    "        r_samples (np.ndarray) - Radial position samples\n",
    "    '''\n",
    "    # First make the icmf interpolator\n",
    "    icmf_xi_interp = _make_icmf_xi_interpolator(denspot,r_min=r_min,\n",
    "        r_max=r_max,a=a)\n",
    "    \n",
    "    # Now draw samples\n",
    "    icmf_samples = np.random.uniform(size=int(n))\n",
    "    xi_samples = icmf_xi_interp(icmf_samples)\n",
    "    return _xi_to_r(xi_samples,a=a)\n",
    "#def\n",
    "\n",
    "def _sample_position_angles(n=1):\n",
    "    '''_sample_position_angles:\n",
    "    \n",
    "    Draw galactocentric, spherical angle samples.\n",
    "    \n",
    "    Args:\n",
    "        n (int) - Number of samples\n",
    "    \n",
    "    Returns:\n",
    "        phi_samples (np.ndarray) - Spherical azimuth\n",
    "        theta_samples (np.ndarray) - Spherical polar angle\n",
    "    '''\n",
    "    phi_samples= np.random.uniform(size=n)*2*np.pi\n",
    "    theta_samples= np.arccos(1.-2*np.random.uniform(size=n))\n",
    "    return phi_samples,theta_samples\n",
    "#def\n",
    "\n",
    "def _make_icmf_xi_interpolator(denspot,r_min=0.,r_max=np.inf,a=1.):\n",
    "    '''_make_icmf_xi_interpolator:\n",
    "\n",
    "    Create the interpolator object which maps the iCMF onto variable xi.\n",
    "    Note - must use self.xi_to_r() on any output of interpolator\n",
    "    Note - the function interpolates the normalized CMF onto the variable \n",
    "    xi defined as:\n",
    "\n",
    "    .. math:: \\\\xi = \\\\frac{r-1}{r+1}\n",
    "\n",
    "    so that xi is in the range [-1,1], which corresponds to an r range of \n",
    "    [0,infinity)\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Returns\n",
    "        icmf_xi_interpolator\n",
    "    '''\n",
    "    xi_min= _r_to_xi(r_min,a=a)\n",
    "    xi_max= _r_to_xi(r_max,a=a)\n",
    "    xis= np.arange(xi_min,xi_max,1e-4)\n",
    "    rs= _xi_to_r(xis,a=a)\n",
    "    \n",
    "    try:\n",
    "        ms = potential.mass(denspot,rs,use_physical=False)\n",
    "    except (AttributeError,TypeError):\n",
    "        ms = np.array([potential.mass(denspot,r,use_physical=False) for r in rs])\n",
    "    ##te\n",
    "    mnorm = potential.mass(denspot,r_max,use_physical=False)\n",
    "\n",
    "    if r_min > 0:\n",
    "        ms -= potential.mass(denspot,r_min,use_physical=False)\n",
    "        mnorm -= potential.mass(denspot,r_min,use_physical=False)\n",
    "    ms /= mnorm\n",
    "        \n",
    "    # Add total mass point\n",
    "    if np.isinf(r_max):\n",
    "        xis= np.append(xis,1)\n",
    "        ms= np.append(ms,1)\n",
    "    return scipy.interpolate.InterpolatedUnivariateSpline(ms,xis,k=3)\n",
    "#def\n",
    "\n",
    "def make_icimf_interpolator(imf,m_min=0.01,m_max=100):\n",
    "    '''make_icimf_interpolator:\n",
    "    Make inverse interpolator for the cumulative initial mass function which \n",
    "    allows you to map normalized (0 to 1) cumulative IMF onto mass \n",
    "    (m_min to m_max). Note that the interpolator maps onto log10(m).\n",
    "    \n",
    "    Args:\n",
    "        imf (callable) - \n",
    "        m_min (float) - minimum mass (must be > 0)\n",
    "        m_max (float) - maximum mass (must be finite)\n",
    "        \n",
    "    Returns:\n",
    "        icimf_interp (scipy.interpolate.InterpolatedUnivariateSpline) - icimf\n",
    "            interpolated spline\n",
    "    '''\n",
    "    assert m_min > 0 and np.isfinite(m_max), 'mass range out of bounds'\n",
    "    ms = np.logspace(np.log10(m_min),np.log10(m_max),1000)\n",
    "    cml_imf = np.empty(len(ms))\n",
    "    for i in range(len(cml_imf)):\n",
    "        cml_imf[i] = cimf(imf,ms[i],a=m_min)\n",
    "    ###i\n",
    "    \n",
    "    # make sure that the cumulative IMF is normalized to 1\n",
    "    cml_imf /= cml_imf[-1]\n",
    "\n",
    "    return scipy.interpolate.InterpolatedUnivariateSpline(cml_imf,np.log10(ms),\n",
    "                                                          k=3)\n",
    "\n",
    "def cimf(f,b,a=0.01,intargs=()):\n",
    "    '''cimf:\n",
    "    Calculate the cumulative of the initial mass function\n",
    "    '''\n",
    "    return scipy.integrate.quad(f,a,b,args=intargs)[0]\n",
    "#def\n",
    "\n",
    "def _r_to_xi(r,a=1.):\n",
    "    '''_r_to_xi:\n",
    "    \n",
    "    Convert r to the variable xi\n",
    "    '''\n",
    "    out= np.divide((r/a-1.),(r/a+1.),where=True^np.isinf(r))\n",
    "    if np.any(np.isinf(r)):\n",
    "        if hasattr(r,'__len__'):\n",
    "            out[np.isinf(r)]= 1.\n",
    "        else:\n",
    "            return 1.\n",
    "    return out\n",
    "#def\n",
    "\n",
    "def _xi_to_r(xi,a=1.):\n",
    "    '''_xi_to_r:\n",
    "    \n",
    "    Convert the variable xi to r\n",
    "    '''\n",
    "    return a*np.divide(1.+xi,1.-xi)\n",
    "#def\n",
    "\n",
    "def _transform_zvecpa(x,y,z,zvec,pa):\n",
    "    '''_transform_zvecpa:\n",
    "    \n",
    "    Transform coordinates using the axis-angle method. First align the\n",
    "    z-axis of the coordinate system with a vector (zvec) and then rotate \n",
    "    about the new z-axis by an angle (pa).\n",
    "    \n",
    "    Args:\n",
    "        x,y,z (array) - Coordinates\n",
    "        zvec (list) - z-axis to align the new coordinate system\n",
    "        pa (float) - Rotation about the transformed z-axis\n",
    "        \n",
    "    Returns:\n",
    "        x_rot,y_rot,z_rot (array) - Rotated coordinates \n",
    "    '''\n",
    "    pa_rot = np.array([[ np.cos(pa), np.sin(pa), 0.],\n",
    "                      [-np.sin(pa), np.cos(pa), 0.],\n",
    "                      [0.         , 0.        , 1.]])\n",
    "\n",
    "    zvec /= np.sqrt(np.sum(zvec**2.))\n",
    "    zvec_rot = _rotate_to_arbitrary_vector(np.array([[0.,0.,1.]]),\n",
    "                                           zvec,inv=True)[0]\n",
    "    R = np.dot(pa_rot,zvec_rot)\n",
    "    \n",
    "    xyz = np.squeeze(np.dstack([x,y,z]))\n",
    "    if np.ndim(xyz) == 1:\n",
    "        xyz_rot = np.dot(R, xyz)\n",
    "        x_rot,y_rot,z_rot = xyz_rot[0],xyz_rot[1],xyz_rot[2]\n",
    "    else:\n",
    "        xyz_rot = np.einsum('ij,aj->ai', R, xyz)\n",
    "        x_rot,y_rot,z_rot = xyz_rot[:,0],xyz_rot[:,1],xyz_rot[:,2]\n",
    "    return x_rot,y_rot,z_rot\n",
    "#def\n",
    "\n",
    "def _transform_alpha_beta_gamma(x,y,z,alpha,beta,gamma):\n",
    "    '''_transform_alpha_beta_gamma:\n",
    "    \n",
    "    Transform x,y,z coordinates by a yaw-pitch-roll transformation.\n",
    "    \n",
    "    Args:\n",
    "        x,y,z (array) - Coordinates\n",
    "        alpha (float) - Roll rotation about the x-axis \n",
    "        beta (float) - Pitch rotation about the transformed y-axis\n",
    "        gamma (float) - Yaw rotation around twice-transformed z-axis\n",
    "        \n",
    "    Returns:\n",
    "        x_rot,y_rot,z_rot (array) - Rotated coordinates \n",
    "    '''\n",
    "    # Roll matrix\n",
    "    Rx = np.zeros([3,3])\n",
    "    Rx[0,0] = 1\n",
    "    Rx[1]   = [0           , np.cos(alpha), -np.sin(alpha)]\n",
    "    Rx[2]   = [0           , np.sin(alpha), np.cos(alpha)]\n",
    "    # Pitch matrix\n",
    "    Ry = np.zeros([3,3])\n",
    "    Ry[0]   = [np.cos(beta), 0            , np.sin(beta)]\n",
    "    Ry[1,1] = 1\n",
    "    Ry[2]   = [-np.sin(beta), 0, np.cos(beta)]\n",
    "    # Yaw matrix\n",
    "    Rz = np.zeros([3,3])\n",
    "    Rz[0]   = [np.cos(gamma), -np.sin(gamma), 0]\n",
    "    Rz[1]   = [np.sin(gamma), np.cos(gamma), 0]\n",
    "    Rz[2,2] = 1\n",
    "    R = np.matmul(Rx,np.matmul(Ry,Rz))\n",
    "    \n",
    "    xyz = np.squeeze(np.dstack([x,y,z]))\n",
    "    if np.ndim(xyz) == 1:\n",
    "        xyz_rot = np.dot(R, xyz)\n",
    "        x_rot,y_rot,z_rot = xyz_rot[0],xyz_rot[1],xyz_rot[2]\n",
    "    else:\n",
    "        xyz_rot = np.einsum('ij,aj->ai', R, xyz)\n",
    "        x_rot,y_rot,z_rot = xyz_rot[:,0],xyz_rot[:,1],xyz_rot[:,2]\n",
    "    return x_rot,y_rot,z_rot\n",
    "#def\n",
    "    \n",
    "### Utilities\n",
    "\n",
    "def load_parsec_isochrone(iso_dir,z,log_age,remove_wd_point=True):\n",
    "    '''load_parsec_isochrone:\n",
    "    \n",
    "    Load a parsec isochrone from a set of old isochrones.\n",
    "        \n",
    "    For old=True the range of metallicities and ages is:\n",
    "    0.0001 <= Z <= 0.0030 in spacing of 0.0001\n",
    "    which equates roughly to -2.28 <= [FE/H] <= -0.8\n",
    "    10 < log Age < 10.15 in spacing of 0.025\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        iso_dir (string) - Directory where the isochrones are located\n",
    "        z (float) - metallicity (will use nearest)\n",
    "        log_age (float) - log_age (will use nearest)\n",
    "        remove_wd_point (bool) - Remove any WD-like points from the isochrone \n",
    "            before matching\n",
    "    '''\n",
    "    # Find which z to use\n",
    "    grid_zs = np.arange(0.0001,0.0031,0.0001)\n",
    "    grid_log_ages = np.arange(10,10.15,0.025)\n",
    "    if z in grid_zs:\n",
    "        z_load = z\n",
    "    else:\n",
    "        z_load = grid_zs[np.argmin(np.abs(z-grid_zs))]\n",
    "        print('Using z='+str(z_load))\n",
    "    ##ie\n",
    "    \n",
    "    # Get filename\n",
    "    iso_name = 'parsec1.2-2mass-spitzer-wise-old'\n",
    "    iso_filename = os.path.join(iso_dir,iso_name,iso_name+\\\n",
    "                                '-Z-{:<06.4}.dat.gz'.format(z_load))\n",
    "    \n",
    "    full_iso = np.genfromtxt(iso_filename, dtype=None, names=True, \n",
    "                             skip_header=11)\n",
    "    \n",
    "    # Find which log Age to use\n",
    "    grid_log_ages = np.unique(full_iso['logAge'])\n",
    "    if log_age in grid_log_ages:\n",
    "        log_age_load = log_age\n",
    "    else:\n",
    "        log_age_load = grid_log_ages[np.argmin(np.abs(log_age-grid_log_ages))]\n",
    "        print('Using log age='+str(log_age_load))\n",
    "    ##ie\n",
    "    \n",
    "    # Extract the isochrone\n",
    "    iso = full_iso[full_iso['logAge']==log_age_load]\n",
    "    \n",
    "    # Remove any points that look like WDs\n",
    "    if remove_wd_point:\n",
    "        wd_inds = np.zeros(len(iso),dtype=bool)\n",
    "        is_wd = True\n",
    "        ind = int(len(iso)-1)\n",
    "        # Start at the end and work backwards until we find the TRGB\n",
    "        while is_wd:\n",
    "            is_wd = (np.diff(iso['Hmag'])[ind-1] > 0.) &\\\n",
    "                    (iso['logg'][ind] > iso['logg'][0])\n",
    "            if is_wd:\n",
    "                wd_inds[ind] = True\n",
    "                ind -= 1\n",
    "            ##fi\n",
    "        ##wh\n",
    "        iso = iso[~wd_inds]\n",
    "    ##fi\n",
    "    \n",
    "    return iso\n",
    "#def\n",
    "\n",
    "def make_fake_allstar(iso,iso_match_indx,locid,fe_h):\n",
    "    '''make_fake_allstar:\n",
    "    \n",
    "    Make a numpy structured array with all the fields required by the density \n",
    "    fitting algorithm to function properly.\n",
    "    \n",
    "    Args:\n",
    "        iso (array) - Isochrone\n",
    "        iso_match_indx (array) - Indices which match samples to isochrone points\n",
    "        locid (array) - location IDs of APOGEE field where each sample lies\n",
    "        fe_h (array) - [Fe/H] abundance of samples\n",
    "    '''\n",
    "    iso_match = iso[iso_match_indx]\n",
    "    atype = np.dtype([('LOCATION_ID', 'i4'),\n",
    "                     ('LOGG', 'f4'),\n",
    "                     ('TEFF', 'f4'),\n",
    "                     ('FE_H', 'f4')\n",
    "                     ])\n",
    "    allstar = np.empty(len(iso_match_indx), dtype=atype)\n",
    "    allstar['LOCATION_ID'] = locid\n",
    "    allstar['LOGG'] = iso_match['logg']\n",
    "    allstar['TEFF'] = iso_match['logTe']\n",
    "    allstar['FE_H'] = fe_h\n",
    "    return allstar\n",
    "#def\n",
    "\n",
    "def join_orbs(orbs):\n",
    "    '''join_orbs:\n",
    "    \n",
    "    Join a list of orbit.Orbit objects together\n",
    "    \n",
    "    '''\n",
    "    for i,o in enumerate(orbs):\n",
    "        if i == 0:\n",
    "            ro = o._ro\n",
    "            vo = o._vo\n",
    "            vxvvs = o._call_internal()\n",
    "        else:\n",
    "            assert ro==o._ro and vo==o._vo, 'ro and/or vo do not match'\n",
    "            vxvvs = np.append(vxvvs, o._call_internal(), axis=1)\n",
    "    ###i\n",
    "    return orbit.Orbit(vxvvs.T,ro=ro,vo=vo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sampling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_DEGTORAD = np.pi/180.\n",
    "\n",
    "def sample_mass(m_tot,imf_type='chabrier',m_min=0.01,m_max=100.):\n",
    "    '''sample_mass:\n",
    "    Draw mass samples from an IMF\n",
    "    \n",
    "    Args:\n",
    "        n (int) - Number of samples to draw\n",
    "        m_tot (float) - Total mass worth of stars to sample\n",
    "        imf_type (string) - IMF type, either chabrier or kroupa\n",
    "        m_min - (float) - minimum mass\n",
    "        m_max - (float) - maximum mass\n",
    "    \n",
    "    Returns:\n",
    "        ms (np.array) - sampled masses (shape n)\n",
    "    '''\n",
    "    ms_for_avg = np.arange(m_min,m_max,0.01)\n",
    "    # First make the icimf interpolator\n",
    "    if imf_type == 'chabrier':\n",
    "        m_avg = np.average(ms_for_avg,weights=chabrier_imf(ms_for_avg))\n",
    "        icimf_interp = make_icimf_interpolator(chabrier_imf,m_min=m_min,\n",
    "                                               m_max=m_max)\n",
    "    elif imf_type == 'kroupa':\n",
    "        m_avg = np.average(ms_for_avg,weights=kroupa_imf(ms_for_avg))\n",
    "        icimf_interp = make_icimf_interpolator(kroupa_imf,m_min=m_min,\n",
    "                                               m_max=m_max)\n",
    "    ##fi\n",
    "    \n",
    "    # Guess how many samples to draw based on the average mass\n",
    "    n_samples_guess = int(m_tot/m_avg)\n",
    "    \n",
    "    # Now draw first round of samples\n",
    "    print('Drawing first samples...')\n",
    "    icimf_samples = np.random.random(n_samples_guess)\n",
    "    ms = np.power(10,icimf_interp(icimf_samples))\n",
    "        \n",
    "    # Add more samples or take some away depending on where things landed\n",
    "    while np.sum(ms) < m_tot:\n",
    "        print('Resampling...')\n",
    "        n_samples_guess = int((m_tot-np.sum(ms))/m_avg)\n",
    "        if n_samples_guess < 1: break\n",
    "        icimf_samples = np.random.random(n_samples_guess)\n",
    "        ms = np.append(ms,np.power(10,icimf_interp(icimf_samples)))\n",
    "    ##wh\n",
    "    if np.sum(ms) > m_tot:\n",
    "        print('Removing some samples...')\n",
    "        ms = ms[:np.where(np.cumsum(ms) > m_tot)[0][0]]\n",
    "    ##fi\n",
    "    \n",
    "    return ms\n",
    "#def\n",
    "\n",
    "def sample_positions(denspot,n=1,r_min=0.,r_max=np.inf,a=None,b=None,c=None,\n",
    "                     zvec=None,pa=None,alpha=None,beta=None,gamma=None,\n",
    "                     return_orbits=False,ro=8.,vo=220.,zo=0.):\n",
    "    '''sample_positions:\n",
    "    \n",
    "    Draw position samples from the density profile. The density profile \n",
    "    must be spherically symmetric. Density profile can be modified to be \n",
    "    triaxial using b and c, and then rotated using either an axis-angle \n",
    "    (zvec and pa) or yaw-pitch-roll (alpha, beta, gamma) methods.\n",
    "    \n",
    "    Args:\n",
    "        denspot (galpy.potential.Potential) - galpy potential representing\n",
    "            the density profile. Must be spherical\n",
    "        n (int) - Number of samples to draw\n",
    "        r_min (float) - \n",
    "        r_max (float) - \n",
    "        a (float) - Density profile scale radius (optional)\n",
    "        b (float) - triaxial y/x scale ratio\n",
    "        c (float) - triaxial z/x scale ratio\n",
    "        zvec (list) - z-axis to align the new coordinate system\n",
    "        pa (float) - Rotation about the transformed z-axis\n",
    "        alpha (float) - Roll rotation about the x-axis \n",
    "        beta (float) - Pitch rotation about the transformed y-axis\n",
    "        gamma (float) - Yaw rotation around twice-transformed z-axis\n",
    "        return_orbits (bool) - Return the orbits\n",
    "    \n",
    "    Returns:\n",
    "        orbs (galpy.orbit.Orbit) - orbits (optional, otherwise None)\n",
    "    '''\n",
    "    \n",
    "    # Draw radial and angular samples\n",
    "    r_samples = _sample_r(denspot,n=n,r_min=r_min,r_max=r_max,a=a)\n",
    "    phi_samples,theta_samples = _sample_position_angles(n=n)\n",
    "    R_samples = r_samples*np.sin(theta_samples)\n",
    "    z_samples = r_samples*np.cos(theta_samples)\n",
    "    \n",
    "    # Apply triaxial scalings and possibly a rotation\n",
    "    if b is not None and c is not None:\n",
    "        x_samples = R_samples*np.cos(phi_samples)\n",
    "        y_samples = R_samples*np.sin(phi_samples)\n",
    "        y_samples *= b\n",
    "        z_samples *= c\n",
    "        if zvec is not None and pa is not None:\n",
    "            x_samples,y_samples,z_samples = _transform_zvecpa(x_samples,\n",
    "                y_samples, z_samples, zvec, pa)\n",
    "        elif alpha is not None and beta is not None and gamma is not None:\n",
    "            x_samples,y_samples,z_samples = _transform_alpha_beta_gamma(\n",
    "                x_samples, y_samples, z_samples, alpha, beta, gamma)\n",
    "        ##ei\n",
    "        R_samples = np.sqrt(x_samples**2.+y_samples**2.)\n",
    "        phi_samples = np.arctan2(y_samples,x_samples)\n",
    "    ##fi\n",
    "    \n",
    "    # Make into orbits\n",
    "    orbs = orbit.Orbit(vxvv=np.array([R_samples,np.zeros(n),np.zeros(n),\n",
    "        z_samples,np.zeros(n),phi_samples]).T,ro=ro,vo=vo,zo=zo)\n",
    "    # self.orbs = orbs\n",
    "    if return_orbits:\n",
    "        return orbs\n",
    "    ##fi\n",
    "#def\n",
    "\n",
    "def apply_APOGEE_selection_function(orbs,ms,aposf,iso,dmap,print_stats=False):\n",
    "    '''apply_APOGEE_selection_function:\n",
    "    \n",
    "    Apply the APOGEE selection function to sample data\n",
    "    \n",
    "    Args:\n",
    "        orbs (galpy.orbit.Orbit) - Orbits representing the samples\n",
    "        ms (np.array) - Masses of the samples\n",
    "        aposf (apogee.select.*) - APOGEE selection function\n",
    "        iso (np.array) - Numpy array\n",
    "        dmap (mwdust.DustMap3D) - Dust map\n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Remove fields without spectroscopic targets\n",
    "    nspec = np.nansum(aposf._nspec_short,axis=1) +\\\n",
    "            np.nansum(aposf._nspec_medium,axis=1) +\\\n",
    "            np.nansum(aposf._nspec_long,axis=1)\n",
    "    good_nspec_fields = np.where(nspec>=1.)[0]\n",
    "    \n",
    "    # Get info about APOGEE pointings\n",
    "    aposf_Hmax = np.dstack([aposf._short_hmax,\n",
    "                            aposf._medium_hmax,\n",
    "                            aposf._long_hmax])[0]\n",
    "    \n",
    "    # Match samples to isochrone entries based on initial mass\n",
    "    t1 = time.time()\n",
    "    init_n = len(ms)\n",
    "    m_err = 1e-4+np.diff(iso['Mini']).max()/2.\n",
    "    good_iso_match,iso_match_indx = _match_isochrone_to_samples(iso,ms,m_err=m_err)\n",
    "    # assert np.all(np.abs(ms[good_iso_match]-iso['Mini'][iso_match_indx]<=m_err))\n",
    "    orbs = orbs[good_iso_match]\n",
    "    ms = ms[good_iso_match]\n",
    "    Hmag = iso['Hmag'][iso_match_indx]\n",
    "    t2 = time.time()\n",
    "    if print_stats:\n",
    "        print(str(len(good_iso_match))+'/'+str(init_n)+\\\n",
    "              ' samples have good matches in the isochrone')\n",
    "        print('Kept '+str(round(100*len(good_iso_match)/init_n,2))+\\\n",
    "              ' % of samples')\n",
    "        print('Matching samples to isochrone entries took '+\\\n",
    "              str(round(t2-t1,1))+' s')\n",
    "    ##fi\n",
    "    init_n = len(ms)\n",
    "    \n",
    "    # Remove samples with apparent Hmag below faintest APOGEE Hmax\n",
    "    t1 = time.time()\n",
    "    dm = 5.*np.log10(orbs.dist().to(apu.pc).value)-5.\n",
    "    where_good_Hmag1 = np.where(np.nanmax(aposf_Hmax) >\\\n",
    "        Hmag + dm)[0]\n",
    "    orbs = orbs[where_good_Hmag1]\n",
    "    ms = ms[where_good_Hmag1]\n",
    "    dm = dm[where_good_Hmag1]\n",
    "    Hmag = Hmag[where_good_Hmag1]\n",
    "    iso_match_indx = iso_match_indx[where_good_Hmag1]\n",
    "    t2=time.time()\n",
    "    if print_stats:\n",
    "        print(str(len(where_good_Hmag1))+'/'+str(init_n)+\\\n",
    "              ' samples are bright enough to be observed')\n",
    "        print('Kept '+str(round(100*len(where_good_Hmag1)/init_n,2))+\\\n",
    "              ' % of samples')\n",
    "        print('Removing samples with H-band magnitudes below faintest APOGEE'+\\\n",
    "              ' Hmax limit took '+str(round(t2-t1,1))+' s')\n",
    "    ##fi\n",
    "    init_n = len(ms)\n",
    "        \n",
    "    # Remove samples that lie outside the APOGEE observational footprint\n",
    "    t1 = time.time()\n",
    "    fp_indx,locid = _remove_orbits_outside_footprint(orbs,aposf,good_nspec_fields)\n",
    "    orbs = orbs[fp_indx]\n",
    "    ms = ms[fp_indx]\n",
    "    dm = dm[fp_indx]\n",
    "    Hmag = Hmag[fp_indx]\n",
    "    iso_match_indx = iso_match_indx[fp_indx]\n",
    "    t2 = time.time()\n",
    "    if print_stats:        \n",
    "        print('Removing samples outside observational footprint took '+\\\n",
    "              str(round(t2-t1,1))+' s')\n",
    "        print(str(len(fp_indx))+'/'+str(init_n)+' samples found within'+\\\n",
    "              ' observational footprint')\n",
    "        print('Kept '+str(round(100*len(fp_indx)/init_n,2))+\\\n",
    "              ' % of samples')\n",
    "    ##fi\n",
    "    init_n = len(ms)\n",
    "    \n",
    "    # Remove samples with apparent Hmag below faintest Hmax on field-by-field \n",
    "    # basis\n",
    "    t1 = time.time()\n",
    "    field_Hmax = np.nanmax(aposf_Hmax, axis=1)\n",
    "    locid_inds = np.where(locid.reshape(locid.size, 1) == aposf._locations)[1]\n",
    "    Hmax = field_Hmax[locid_inds]\n",
    "    where_good_Hmag2 = np.where(Hmax > Hmag + dm)[0]\n",
    "    # pdb.set_trace()\n",
    "    # Access values with good H-magnitudes\n",
    "    orbs = orbs[where_good_Hmag2]\n",
    "    locid = locid[where_good_Hmag2]\n",
    "    ms = ms[where_good_Hmag2]\n",
    "    dm = dm[where_good_Hmag2]\n",
    "    Hmag = Hmag[where_good_Hmag2]\n",
    "    iso_match_indx = iso_match_indx[where_good_Hmag2]\n",
    "    Jmag = iso['Jmag'][iso_match_indx]\n",
    "    Ksmag = iso['Ksmag'][iso_match_indx]\n",
    "    t2=time.time()\n",
    "    if print_stats:\n",
    "        print('Removing samples with H-band magnitudes outside '+\\\n",
    "              'observational limits took '+str(round(t2-t1,1))+' s')\n",
    "        print(str(len(where_good_Hmag2))+'/'+str(init_n)+\\\n",
    "                  ' samples are bright enough to be observed')\n",
    "        print('Kept '+str(round(100*len(where_good_Hmag2)/init_n,2))+\\\n",
    "              ' % of samples')\n",
    "    ##fi\n",
    "    init_n = len(ms)\n",
    "        \n",
    "    # Get lbIndx for the dust map\n",
    "    t1 = time.time()\n",
    "    gl = orbs.ll(use_physical=True).value\n",
    "    gb = orbs.bb(use_physical=True).value\n",
    "    dist = np.atleast_2d(orbs.dist(use_physical=True).value).T\n",
    "    # Information about the dust map\n",
    "    dmap_nsides = np.array(dmap._nsides)\n",
    "    nside_pix = np.zeros((len(orbs),len(dmap_nsides)))\n",
    "    nside_arr = np.repeat(dmap_nsides[:,np.newaxis],len(orbs),axis=1).T\n",
    "    for i in range(len(dmap_nsides)):\n",
    "        nside_pix[:,i] = healpy.pixelfunc.ang2pix(dmap_nsides[i],\n",
    "                                                  (90.-gb)*_DEGTORAD,\n",
    "                                                   gl*_DEGTORAD, nest=True)\n",
    "    # Calculate healpix u\n",
    "    dmap_hpu = (dmap._pix_info['healpix_index'] + 4*dmap._pix_info['nside']**2.).astype(int)\n",
    "    hpu = (nside_pix + 4*nside_arr**2).astype(int)\n",
    "    # Use searchsorted to get the indices\n",
    "    dmap_hpu_argsort = np.argsort(dmap_hpu)\n",
    "    dmap_hpu_sorted = dmap_hpu[dmap_hpu_argsort]\n",
    "    hpu_indx_sorted = np.searchsorted(dmap_hpu_sorted,hpu)\n",
    "    hpu_indx = np.take(dmap_hpu_argsort, hpu_indx_sorted, mode=\"clip\")\n",
    "    hpu_mask = dmap_hpu[hpu_indx] != hpu\n",
    "    hpu_ma = np.ma.array(hpu_indx, mask=hpu_mask)\n",
    "    # Check if somehow a sample has no matches or more than one match. The \n",
    "    # former should not happen ever, the later has been observed.\n",
    "    hpu_ma_sum = np.sum(~hpu_ma.mask,axis=1)\n",
    "    lbIndx = np.zeros(len(orbs))\n",
    "  \n",
    "    # lbIndx = hpu_ma.data[~hpu_ma.mask]\n",
    "    if np.any(hpu_ma_sum==0):\n",
    "        # Need to code in a solution\n",
    "        raise RuntimeError('A sample did not find a lbIndx in the dust map')\n",
    "    hpu_ma_multi = hpu_ma_sum>1\n",
    "    if np.any(hpu_ma_multi):\n",
    "        print('Warning: At least one sample has more than one match at'\\\n",
    "              +'different Nside, choosing the highest resolution matches')\n",
    "        lbIndx[~hpu_ma_multi] = hpu_ma.data[~hpu_ma_multi][~hpu_ma.mask[~hpu_ma_multi]]\n",
    "        where_hpu_ma_multi = np.where(hpu_ma_multi)[0]\n",
    "        for i in range(len(where_hpu_ma_multi)):\n",
    "            multi_ind = where_hpu_ma_multi[i]\n",
    "            # Choose the highest resolution index\n",
    "            lbIndx[multi_ind] = hpu_ma.data[multi_ind][~hpu_ma.mask[multi_ind]][0]\n",
    "    else:\n",
    "        lbIndx = hpu_ma.data[~hpu_ma.mask]\n",
    "    \n",
    "    t2 = time.time()\n",
    "    if print_stats:\n",
    "        print('Getting lbIndx took '+str(round(t2-t1,1))+' s')\n",
    "    ##fi\n",
    "        \n",
    "    # Compute AH\n",
    "    t1 = time.time()\n",
    "    unique_lbIndx = np.unique(lbIndx).astype(int)\n",
    "    AH = np.zeros(len(orbs))\n",
    "    for i in range(len(unique_lbIndx)):\n",
    "        # First find which samples have this lbIndx\n",
    "        where_unique = np.where(lbIndx == unique_lbIndx[i])[0]\n",
    "        # Get the dust map interpolation data for this lbIndx\n",
    "        dmap_interp_data = scipy.interpolate.InterpolatedUnivariateSpline(\n",
    "            dmap._distmods, dmap._best_fit[unique_lbIndx[i]], k=dmap._interpk)\n",
    "        # Calcualate AH\n",
    "        eBV_to_AH = mwdust.util.extCurves.aebv(dmap._filter,sf10=dmap._sf10)\n",
    "        AH[where_unique] = dmap_interp_data(dm[where_unique])*eBV_to_AH\n",
    "    ###i\n",
    "    t2 = time.time()\n",
    "#     if print_stats:\n",
    "#         print('Getting AH took '+str(t2-t1)+' s')\n",
    "#     ##fi\n",
    "    \n",
    "    # Apply the selection function\n",
    "    t1 = time.time()\n",
    "    sf_keep_indx = np.zeros(len(orbs),dtype=bool)\n",
    "    for i in range(len(orbs)):\n",
    "        random_n = np.random.random(size=1)[0]\n",
    "        _H = Hmag[i] + dm[i] + AH[i]\n",
    "        _JK0 = Jmag[i] - Ksmag[i]\n",
    "        compare_n = aposf(locid[i],_H,_JK0)\n",
    "        if compare_n > random_n: sf_keep_indx[i] = True\n",
    "    t2 = time.time()\n",
    "    if print_stats:\n",
    "        print('Applying selection function took '+str(round(t2-t1,1))+' s')\n",
    "        print(str(np.sum(sf_keep_indx))+'/'+str(init_n)+\\\n",
    "                  ' samples survive the selection function')\n",
    "        print('Kept '+str(round(100*np.sum(sf_keep_indx)/init_n,2))+\\\n",
    "              ' % of samples')\n",
    "    ##fi\n",
    "    \n",
    "    orbs = orbs[sf_keep_indx]\n",
    "    locid = locid[sf_keep_indx]\n",
    "    ms = ms[sf_keep_indx]\n",
    "    iso_match_indx = iso_match_indx[sf_keep_indx]\n",
    "    \n",
    "    return orbs, locid, ms, iso_match_indx\n",
    "#def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load selection function, dust map, isochrone, mock orbits and masses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aposf_data_dir = '/geir_data/scr/lane/projects/ges-mass/data/gaia_apogee/'+\\\n",
    "                 'apogee_dr16_l33_gaia_dr2/'\n",
    "with open(aposf_data_dir+'apogee_SF.dat','rb') as f:\n",
    "    aposf = pickle.load(f)\n",
    "##wi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmap = mwdust.Combined19(filter='2MASS H') # dustmap from mwdust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_parsec_1_2_iso_keys = {'mass_initial':'Mini',\n",
    "                        'z_initial':'Zini',\n",
    "                        'log_age':'logAge',\n",
    "                        'jmag':'Jmag',\n",
    "                        'hmag':'Hmag',\n",
    "                        'ksmag':'Ksmag',\n",
    "                        'logg':'logg',\n",
    "                        'logteff':'logTe'\n",
    "                        }\n",
    "z = 0.0010\n",
    "log_age = 10.0\n",
    "iso_grid = np.load('/geir_data/scr/lane/projects/ges-mass/data/gaia_apogee/apogee_dr16_l33_gaia_dr2/iso_grid.npy')\n",
    "iso = iso_grid[(iso_grid['Zini']==z) & (iso_grid['logAge']==log_age)]\n",
    "iso = iso[iso['logL']>-9.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data/iso_z_'+str(z)+'_log_age_'+str(log_age)+'.npy',iso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the APOGEEMock class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 2.5\n",
    "denspot_args = {'alpha':alpha}\n",
    "denspot = potential.PowerSphericalPotential(**denspot_args, ro=ro, vo=vo)\n",
    "\n",
    "fallstar = []\n",
    "orbs = []\n",
    "m_tot = 2e6\n",
    "n_chunk = 2\n",
    "\n",
    "for i in range(n_chunk):\n",
    "    mock = apomock.APOGEEMock(denspot, ro=ro, vo=vo)\n",
    "    mock.load_isochrone(iso=iso, iso_keys=_parsec_1_2_iso_keys)\n",
    "\n",
    "    print('Sampling masses')\n",
    "    t1 = time.time()\n",
    "    \n",
    "    m_min = 0.08\n",
    "    mock.sample_masses(m_tot/n_chunk, m_min=m_min)\n",
    "    t2 = time.time()\n",
    "    print('Took '+str(t2-t1)+' s')\n",
    "\n",
    "    print('Sampling positions')\n",
    "    t1 = time.time()\n",
    "    r_min = 2./ro\n",
    "    r_max = 70./ro\n",
    "    mock.sample_positions(r_min=r_min, r_max=r_max)\n",
    "    t2 = time.time()\n",
    "    print('Took '+str(t2-t1)+' s')\n",
    "\n",
    "    print('Applying selection function')\n",
    "    t1 = time.time()\n",
    "    mock.apply_selection_function(aposf, dmap)\n",
    "    t2 = time.time()\n",
    "    print('Took '+str(t2-t1)+' s')\n",
    "\n",
    "    fallstar.append( mock.make_allstar() )\n",
    "    orbs.append( mock.orbs )\n",
    "    if i+1 < n_chunk:\n",
    "        del mock\n",
    "    gc.collect()\n",
    "\n",
    "print('Found '+str(len(fallstar))+' samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = mock._write_mock_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_summary.txt','w') as f:\n",
    "    for line in summary:\n",
    "        f.write(line)\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_number = '5'\n",
    "with open('./data/mock_'+mock_number+'/orbs.pkl','wb') as f:\n",
    "    pickle.dump(orbs,f)\n",
    "##wi\n",
    "np.save('./data/mock_'+mock_number+'/allstar.npy',fallstar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut bulge fields. Within 20 degrees of the galactic center\n",
    "omask_bulge = ~(((orbs.ll().value > 340.) |\\\n",
    "                (orbs.ll().value < 20.)) &\\\n",
    "               (np.fabs(orbs.bb().value) < 20.)\n",
    "              )\n",
    "# Cut fields containing enhancements of globular cluster stars\n",
    "gc_locid = [2011,4353,5093,5229,5294,5295,5296,5297,5298,5299,5300,5325,5328,\n",
    "            5329,5438,5528,5529,5744,5801]\n",
    "omask_gc = ~np.isin(fallstar['LOCATION_ID'],gc_locid)\n",
    "omask_logg = (fallstar['LOGG'] > 1.) & (fallstar['LOGG'] < 3.)\n",
    "\n",
    "omask = omask_bulge & omask_gc & omask_logg\n",
    "print(str(np.sum(omask))+' samples survived observational masking')\n",
    "\n",
    "np.save('./data/mock_'+mock_number+'/omask.npy',omask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do it the way with more steps with the old routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtot = 1e8\n",
    "t1 = time.time()\n",
    "ms = sample_mass(mtot,m_min=0.08,m_max=iso['Mini'].max())\n",
    "t2 = time.time()\n",
    "print('Took '+str(round(t2-t1,1))+' s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "# rc = 30./ro\n",
    "alpha = 2.0\n",
    "denspot_args = {'alpha':alpha}\n",
    "denspot = potential.PowerSphericalPotential(**denspot_args)\n",
    "\n",
    "potential.turn_physical_off(denspot)\n",
    "r_min = 2./ro\n",
    "r_max = 70./ro\n",
    "a = 1.\n",
    "b=1.\n",
    "c=1.\n",
    "rot_zvec = np.array([0.,0.,1.])\n",
    "rot_pa = 0.\n",
    "rot_alpha = 0.\n",
    "rot_beta = 0.\n",
    "rot_gamma = 0.\n",
    "orbs = sample_positions(denspot,n=len(ms),r_min=r_min,r_max=r_max,a=a,\n",
    "                        b=b,c=c,zvec=None,pa=None,return_orbits=True,\n",
    "                        ro=ro,vo=vo,zo=zo)\n",
    "t2 = time.time()\n",
    "print('Took '+str(round(t2-t1,1))+' s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbs_sf,locid,ms_sf,iso_match_indx = apply_APOGEE_selection_function(\n",
    "                                        orbs,ms,aposf,iso,dmap,\n",
    "                                        print_stats=True)\n",
    "print('Found '+str(len(orbs_sf))+' samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallstar = make_fake_allstar(iso,iso_match_indx,locid,\n",
    "                              np.ones(len(orbs_sf))*Z2FEH(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_number = '5'\n",
    "with open('./data/mock_'+mock_number+'/orbs.pkl','wb') as f:\n",
    "    pickle.dump(orbs_sf,f)\n",
    "##wi\n",
    "np.save('./data/mock_'+mock_number+'/allstar.npy',fallstar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the obsevational masks for fitting\n",
    "Exclude stars which:\n",
    "1. Lie within a 20 degree square around the bulge\n",
    "2. Lie within a field containing a globular cluster\n",
    "3. Have log(g) > 3 or log(g) < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut bulge fields. Within 20 degrees of the galactic center\n",
    "omask_bulge = ~(((orbs_sf.ll().value > 340.) |\\\n",
    "                (orbs_sf.ll().value < 20.)) &\\\n",
    "               (np.fabs(orbs_sf.bb().value) < 20.)\n",
    "              )\n",
    "# Cut fields containing enhancements of globular cluster stars\n",
    "gc_locid = [2011,4353,5093,5229,5294,5295,5296,5297,5298,5299,5300,5325,5328,\n",
    "            5329,5438,5528,5529,5744,5801]\n",
    "omask_gc = ~np.isin(fallstar['LOCATION_ID'],gc_locid)\n",
    "omask_logg = (fallstar['LOGG'] > 1.) & (fallstar['LOGG'] < 3.)\n",
    "\n",
    "omask = omask_bulge & omask_gc & omask_logg\n",
    "print(str(np.sum(omask))+' samples survived observational masking')\n",
    "\n",
    "np.save('./data/mock_'+mock_number+'/omask.npy',omask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch compute for large numbers of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total mass and batch size\n",
    "mtot = 2e8\n",
    "batch_m = 5e7\n",
    "batch_n = int(mtot / batch_m)\n",
    "\n",
    "# Potential\n",
    "# rc = 30./ro\n",
    "alpha = 2.0\n",
    "denspot_args = {'alpha':alpha}\n",
    "denspot = potential.PowerSphericalPotential(**denspot_args)\n",
    "potential.turn_physical_off(denspot)\n",
    "\n",
    "# Position sampling\n",
    "r_min = 2./ro\n",
    "r_max = 50./ro\n",
    "a = 1.\n",
    "b=0.8\n",
    "c=0.5\n",
    "rot_zvec = np.array([0.,0.,1.])\n",
    "rot_pa = np.pi/4\n",
    "rot_alpha = 0.\n",
    "rot_beta = 0.\n",
    "rot_gamma = 0.\n",
    "\n",
    "orbs_b = []\n",
    "ms_b = []\n",
    "locid_b = []\n",
    "iso_match_indx_b = []\n",
    "\n",
    "for i in range(batch_n):\n",
    "    batch_mtot = mtot / batch_n\n",
    "    print('\\nDoing batch '+str(i+1)+', mass: '+str(batch_mtot/1e8)+'e8 Msun')\n",
    "    \n",
    "    t1 = time.time()\n",
    "    ms = sample_mass(batch_mtot,m_min=0.09,m_max=0.87)\n",
    "    t2 = time.time()\n",
    "    print('Masses took '+str(round(t2-t1,1))+' s')\n",
    "    \n",
    "    t1 = time.time()\n",
    "    orbs = sample_positions(denspot,n=len(ms),r_min=r_min,r_max=r_max,a=a,\n",
    "                        b=b,c=c,zvec=rot_zvec,pa=rot_pa,return_orbits=True,\n",
    "                        ro=ro,vo=vo,zo=zo)\n",
    "    t2 = time.time()\n",
    "    print('Orbits took '+str(round(t2-t1,1))+' s')\n",
    "    \n",
    "    t1 = time.time()\n",
    "    orbs_sf,locid,ms_sf,iso_match_indx = apply_APOGEE_selection_function(\n",
    "                                        orbs,ms,aposf,iso,dmap,print_stats=False)\n",
    "    t2 = time.time()\n",
    "    print('SF application took '+str(round(t2-t1,1))+' s')\n",
    "    \n",
    "    orbs_b.append(orbs_sf)\n",
    "    ms_b.append(ms_sf)\n",
    "    locid_b.append(locid)\n",
    "    iso_match_indx_b.append(iso_match_indx)\n",
    "    \n",
    "    del orbs,ms\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_sf = np.concatenate(ms_b)\n",
    "locid = np.concatenate(locid_b)\n",
    "iso_match_indx = np.concatenate(iso_match_indx_b)\n",
    "orbs_sf = join_orbs(orbs_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallstar = make_fake_allstar(iso,iso_match_indx,locid,\n",
    "                              np.ones(len(orbs_sf))*Z2FEH(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_number = '4'\n",
    "with open('./data/mock_'+mock_number+'/orbs.pkl','wb') as f:\n",
    "    pickle.dump(orbs_sf,f)\n",
    "##wi\n",
    "np.save('./data/mock_'+mock_number+'/allstar.npy',fallstar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut bulge fields. Within 20 degrees of the galactic center\n",
    "omask_bulge = ~(((orbs_sf.ll().value > 340.) |\\\n",
    "                (orbs_sf.ll().value < 20.)) &\\\n",
    "               (np.fabs(orbs_sf.bb().value) < 20.)\n",
    "              )\n",
    "\n",
    "# Cut fields containing enhancements of globular cluster stars\n",
    "gc_locid = [2011,4353,5093,5229,5294,5295,5296,5297,5298,5299,5300,5325,5328,\n",
    "            5329,5438,5528,5529,5744,5801]\n",
    "omask_gc = ~np.isin(fallstar['LOCATION_ID'],gc_locid)\n",
    "\n",
    "omask_logg = (fallstar['LOGG'] > 1.) & (fallstar['LOGG'] < 3.)\n",
    "\n",
    "omask = omask_bulge & omask_gc & omask_logg\n",
    "\n",
    "np.save('./data/mock_'+mock_number+'/omask.npy',omask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
