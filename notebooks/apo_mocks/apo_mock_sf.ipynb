{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "#\n",
    "# TITLE - apo_mock_sf\n",
    "# AUTHOR - James Lane\n",
    "# PROJECT - ges-mass\n",
    "#\n",
    "# ------------------------------------------------------------------------\n",
    "#\n",
    "# Docstrings and metadata:\n",
    "'''Create the framework to generate mock APOGEE realizations of the stellar \n",
    "halo that can be used to test fitting routines. Investigate selection function\n",
    "'''\n",
    "\n",
    "__author__ = \"James Lane\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "\n",
    "## Basic\n",
    "import numpy as np\n",
    "import sys, os, pdb, copy, glob, subprocess, warnings, dill as pickle, time\n",
    "from astropy import units as apu\n",
    "\n",
    "## Matplotlib\n",
    "# import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "## galpy\n",
    "from galpy import orbit\n",
    "from galpy import potential\n",
    "# from galpy import actionAngle as aA\n",
    "from galpy import df\n",
    "from galpy.util import _rotate_to_arbitrary_vector\n",
    "\n",
    "## APOGEE, isochrones, dustmaps\n",
    "from apogee import select as apsel\n",
    "from isodist import Z2FEH,FEH2Z\n",
    "import mwdust\n",
    "from mwdust.util.extCurves import aebv\n",
    "\n",
    "## scipy\n",
    "import scipy.integrate\n",
    "import scipy.interpolate\n",
    "\n",
    "## Astropy and healpix\n",
    "from astropy.coordinates import SkyCoord\n",
    "import healpy\n",
    "\n",
    "## Project-specific\n",
    "sys.path.insert(0,'../../../src/')\n",
    "# import sample_project.module as project_module\n",
    "\n",
    "### Scale parameters\n",
    "ro = 8.275\n",
    "vo = 220\n",
    "zo = 0.0208 # Bennett+ 2019\n",
    "\n",
    "### Notebook setup\n",
    "%matplotlib inline\n",
    "plt.style.use('../../../src/mpl/project.mplstyle') # This must be exactly here\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plan\n",
    "For the selection function portion of the algorithm first winnow down the number \n",
    "of samples as much as possible:\n",
    "1. Remove fields without any spectroscopic targets\n",
    "2. Remove samples outside the observational footprint\n",
    "3. Use the supplied isochrones to remove samples with distance modulus-adjust H-band magnitudes below the survey limit\n",
    "\n",
    "Then actually apply the main part of the algorithm\n",
    "1. Calculate the H-band extinction using the dust map\n",
    "2. Apply the selection function to the redenning-adjusted samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # These are the usual ipython objects, including this one you are creating\n",
    "# ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# # Get a sorted list of the objects and their sizes\n",
    "# sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() \\\n",
    "#     if not x.startswith('_') and x not in sys.modules \\\n",
    "#     and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_parsec_1_2_iso_keys = {'mass_initial':'Mini',\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _remove_dead_stars(ms,iso,iso_keys=_parsec_1_2_iso_keys):\n",
    "#     '''_remove_dead_stars:\n",
    "    \n",
    "#     Remove stars with masses greater than the highest initial mass present \n",
    "#     in the isochrone.\n",
    "    \n",
    "#     Args:\n",
    "#         ms (array) - array of masses\n",
    "#         iso (array) - isochrone array\n",
    "#         iso_keys (array) - \n",
    "    \n",
    "#     Returns:\n",
    "#         as_indx (array) - index of 'alive' stars to consider\n",
    "#     '''\n",
    "#     return np.where(ms < np.nanmax(iso[iso_keys['mass_initial']]))[0]\n",
    "# #def \n",
    "    \n",
    "\n",
    "def _remove_orbits_outside_footprint(orbs,aposf,field_indx=None,\n",
    "                                     chunk_size=None):\n",
    "    '''_remove_orbits_outside_footprint:\n",
    "    \n",
    "    Remove stellar samples from outside the APOGEE observational footprint.\n",
    "    Each plate has a variable field of view, and an inner 'hole' of \n",
    "    5 arcminutes. Using field_indx allows for selecting only a subset of the \n",
    "    available fields to use.\n",
    "    \n",
    "    Args:\n",
    "        orbs (array) - Orbits representing samples\n",
    "        aposf (array) - APOGEE selection function\n",
    "        field_indx (array) - Indices of fields to consider\n",
    "        chunk_size (int) - If not None, split up orbs into chunks of this \n",
    "            size to \n",
    "    \n",
    "    Returns:\n",
    "        fp_indx () - Index of samples that lie within the observational\n",
    "            footprint\n",
    "        fp_locid () - Location IDs of field each sample lies within\n",
    "    '''\n",
    "    # Account for field_indx, fields we want to consider\n",
    "    if field_indx is None:\n",
    "        field_indx = np.arange(0,len(aposf._apogeeFields),dtype=int)\n",
    "    ##fi\n",
    "    \n",
    "    # field center coordinates, location IDs, radii\n",
    "    glon = aposf._apogeeField['GLON'][field_indx]\n",
    "    glat = aposf._apogeeField['GLAT'][field_indx]\n",
    "    locids = aposf._locations[field_indx]\n",
    "    radii = np.zeros(len(field_indx))\n",
    "    for i in range(len(locids)):\n",
    "        radii[i] = aposf.radius(locids[i])\n",
    "    ###i\n",
    "    \n",
    "    # Make SkyCoord objects\n",
    "    aposf_sc = SkyCoord(frame='galactic', l=glon*apu.deg, b=glat*apu.deg)\n",
    "    orbs_sc = SkyCoord(frame='galactic', l=orbs.ll(), b=orbs.bb())\n",
    "    \n",
    "    # First nearest-neighbor match\n",
    "    indx,sep,_ = orbs_sc.match_to_catalog_sky(aposf_sc)\n",
    "    indx_radii = radii[indx]\n",
    "    indx_locid = locids[indx]\n",
    "    fp_indx = np.where(np.logical_and(sep < indx_radii*apu.deg,\n",
    "                                      sep > 5.5*apu.arcmin))[0]\n",
    "    fp_locid = indx_locid[fp_indx]\n",
    "    \n",
    "    # Second nearest-neighbor match for samples inside plate central holes\n",
    "    where_in_hole = np.where(sep < 5.5*apu.arcmin)[0]\n",
    "    indx2,sep2,_ = orbs_sc[where_in_hole].match_to_catalog_sky(aposf_sc,\n",
    "                                                               nthneighbor=2)\n",
    "    indx2_radii = radii[indx2]\n",
    "    indx2_locid = locids[indx2]\n",
    "    fp_indx2 = np.where(np.logical_and(sep2 < indx2_radii*apu.deg,\n",
    "                                       sep2 > 5.5*apu.arcmin))[0]\n",
    "    if len(fp_indx2) > 0:\n",
    "        fp_indx = np.append(fp_indx,where_in_hole[fp_indx2])\n",
    "        fp_locid = np.append(fp_locid,indx2_locid[fp_indx2])\n",
    "    ##fi\n",
    "    \n",
    "    return fp_indx,fp_locid\n",
    "#def\n",
    "\n",
    "def _match_isochrone_to_samples(iso,ms,m_err=0.05,iso_keys=_parsec_1_2_iso_keys):\n",
    "    '''_match_isochrone_to_samples:\n",
    "    \n",
    "    Match the samples to entries in an isochrone according to initial mass\n",
    "    \n",
    "    iso_keys must accept the following keys:\n",
    "    'Mini' -> initial mass key\n",
    "    \n",
    "    Args:\n",
    "        iso (array) - isochrone array\n",
    "        ms (array) - sample masses\n",
    "        m_err (float) - Maximum difference in mass between sample and isochrone\n",
    "            for successful match\n",
    "        iso_keys (dict) - Dictionary of keys for accessing the isochrone \n",
    "            properties, accessible via a common set of strings (see above)\n",
    "        \n",
    "    Returns:\n",
    "        good_match (array) - Indices of ms which found matches in the isochrone \n",
    "            array within m_err tolerance\n",
    "        match_indx (array) - array of matches, length len(good_match), \n",
    "            indexing ms into iso\n",
    "    '''\n",
    "    # Access initial mass\n",
    "    mass_initial_key = iso_keys['mass_initial']\n",
    "    m0 = iso[mass_initial_key]\n",
    "    \n",
    "    # Ensure isochrone is sorted by initial mass\n",
    "    m0_argsort = np.argsort(m0)\n",
    "    m0_sorted = m0[m0_argsort]\n",
    "    \n",
    "    # Search the sorted array for the nearest neighbors (fast)\n",
    "    m0_mids = m0_sorted[1:] - np.diff(m0_sorted.astype('f'))/2\n",
    "    idx = np.searchsorted(m0_mids, ms)\n",
    "    cand_indx = m0_argsort[idx]\n",
    "    residual = ms - m0_sorted[cand_indx]\n",
    "    \n",
    "    good_match = np.where( (np.abs(residual) < m_err) &\\\n",
    "                           (ms < m0[-1]) &\\\n",
    "                           (ms > m0[0])\n",
    "                          )[0]\n",
    "\n",
    "    match_indx = np.argsort(m0_argsort)[cand_indx[good_match]]\n",
    "\n",
    "    np.all(np.abs(ms[good_match]-m0[match_indx]) <= m_err)\n",
    "    \n",
    "    return good_match,match_indx\n",
    "#def\n",
    "\n",
    "def _remove_faint_Hmag_stars(Hmag,aposf):\n",
    "    '''_remove_faint_Hmag_stars:\n",
    "    \n",
    "    Remove stars with Hmag below the limit for each individual field\n",
    "    '''\n",
    "    # Find maximum Hmag values for all cohorts in each field and compare to the \n",
    "    # distance modulus-adjusted value for each sample\n",
    "    field_Hmax = np.nanmax(np.dstack([aposf._short_hmax,\n",
    "                                      aposf._medium_hmax,\n",
    "                                      aposf._long_hmax])[0],axis=1)\n",
    "    dm = 5.*np.log10(im_orbs.dist().to(apu.pc).value)-5.\n",
    "    im_Hmag_app = im_Hmag + dm\n",
    "    im_locid_inds = np.where(im_locid.reshape(im_locid.size, 1) == aposf._locations)[1]\n",
    "    im_Hmax = field_Hmax[im_locid_inds]\n",
    "    where_good_Hmag = np.where(im_Hmax > im_Hmag_app)[0]\n",
    "    print(str(len(where_good_Hmag))+'/'+str(len(im_Hmag_app))+\\\n",
    "              ' samples are bright enough to be observed')\n",
    "\n",
    "    # Access values with good H-magnitudes\n",
    "    gh_orbs = im_orbs[where_good_Hmag]\n",
    "    gh_locid = im_locid[where_good_Hmag]\n",
    "    gh_ms = im_ms[where_good_Hmag]\n",
    "    gh_Hmag_app = im_Hmag_app[where_good_Hmag]\n",
    "    \n",
    "    \n",
    "### Utilities\n",
    "\n",
    "def load_parsec_isochrone(iso_dir,z,log_age,remove_wd_point=True):\n",
    "    '''load_parsec_isochrone:\n",
    "    \n",
    "    Load a parsec isochrone from a set of old isochrones.\n",
    "        \n",
    "    For old=True the range of metallicities and ages is:\n",
    "    0.0001 <= Z <= 0.0030 in spacing of 0.0001\n",
    "    which equates roughly to -2.28 <= [FE/H] <= -0.8\n",
    "    10 < log Age < 10.15 in spacing of 0.025\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        iso_dir (string) - Directory where the isochrones are located\n",
    "        z (float) - metallicity (will use nearest)\n",
    "        log_age (float) - log_age (will use nearest)\n",
    "        remove_wd_point (bool) - Remove any WD-like points from the isochrone \n",
    "            before matching\n",
    "    '''\n",
    "    # Find which z to use\n",
    "    grid_zs = np.arange(0.0001,0.0031,0.0001)\n",
    "    grid_log_ages = np.arange(10,10.15,0.025)\n",
    "    if z in grid_zs:\n",
    "        z_load = z\n",
    "    else:\n",
    "        z_load = grid_zs[np.argmin(np.abs(z-grid_zs))]\n",
    "        print('Using z='+str(z_load))\n",
    "    ##ie\n",
    "    \n",
    "    # Get filename\n",
    "    iso_name = 'parsec1.2-2mass-spitzer-wise-old'\n",
    "    iso_filename = os.path.join(iso_dir,iso_name,iso_name+\\\n",
    "                                '-Z-{:<06.4}.dat.gz'.format(z_load))\n",
    "    \n",
    "    full_iso = np.genfromtxt(iso_filename, dtype=None, names=True, \n",
    "                             skip_header=11)\n",
    "    \n",
    "    # Find which log Age to use\n",
    "    grid_log_ages = np.unique(full_iso['logAge'])\n",
    "    if log_age in grid_log_ages:\n",
    "        log_age_load = log_age\n",
    "    else:\n",
    "        log_age_load = grid_log_ages[np.argmin(np.abs(log_age-grid_log_ages))]\n",
    "        print('Using log age='+str(log_age_load))\n",
    "    ##ie\n",
    "    \n",
    "    # Extract the isochrone\n",
    "    iso = full_iso[full_iso['logAge']==log_age_load]\n",
    "    \n",
    "    # Remove any points that look like WDs\n",
    "    if remove_wd_point:\n",
    "        wd_inds = np.zeros(len(iso),dtype=bool)\n",
    "        is_wd = True\n",
    "        ind = int(len(iso)-1)\n",
    "        # Start at the end and work backwards until we find the TRGB\n",
    "        while is_wd:\n",
    "            is_wd = (np.diff(iso['Hmag'])[ind-1] > 0.) &\\\n",
    "                    (iso['logg'][ind] > iso['logg'][0])\n",
    "            if is_wd:\n",
    "                wd_inds[ind] = True\n",
    "                ind -= 1\n",
    "            ##fi\n",
    "        ##wh\n",
    "        iso = iso[~wd_inds]\n",
    "    ##fi\n",
    "    \n",
    "    return iso\n",
    "#def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_APOGEE_selection_function(orbs,ms,aposf,iso,dmap,print_stats=False):\n",
    "    '''apply_APOGEE_selection_function:\n",
    "    \n",
    "    Apply the APOGEE selection function to sample data\n",
    "    \n",
    "    Args:\n",
    "        orbs (galpy.orbit.Orbit) - Orbits representing the samples\n",
    "        ms (np.array) - Masses of the samples\n",
    "        aposf (apogee.select.*) - APOGEE selection function\n",
    "        iso (np.array) - Numpy array\n",
    "        dmap (mwdust.DustMap3D) - Dust map\n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Remove fields without spectroscopic targets\n",
    "    nspec = np.nansum(aposf._nspec_short,axis=1) +\\\n",
    "            np.nansum(aposf._nspec_medium,axis=1) +\\\n",
    "            np.nansum(aposf._nspec_long,axis=1)\n",
    "    good_nspec_fields = np.where(nspec>=1.)[0]\n",
    "    \n",
    "    # Get info about APOGEE pointings\n",
    "    aposf_Hmax = np.dstack([aposf._short_hmax,\n",
    "                            aposf._medium_hmax,\n",
    "                            aposf._long_hmax])[0]\n",
    "    \n",
    "    # Match samples to isochrone entries based on initial mass\n",
    "    t1 = time.time()\n",
    "    init_n = len(ms)\n",
    "    m_err = 1e-4+np.diff(iso['Mini']).max()/2.\n",
    "    good_iso_match,iso_match_indx = _match_isochrone_to_samples(iso,ms,m_err=m_err)\n",
    "    # assert np.all(np.abs(ms[good_iso_match]-iso['Mini'][iso_match_indx]<=m_err))\n",
    "    orbs = orbs[good_iso_match]\n",
    "    ms = ms[good_iso_match]\n",
    "    Hmag = iso['Hmag'][iso_match_indx]\n",
    "    t2 = time.time()\n",
    "    if print_stats:\n",
    "        print(str(len(good_iso_match))+'/'+str(init_n)+\\\n",
    "              ' samples have good matches in the isochrone')\n",
    "        print('Kept '+str(round(100*len(good_iso_match)/init_n,2))+\\\n",
    "              ' % of samples')\n",
    "        print('Matching samples to isochrone entries took '+\\\n",
    "              str(round(t2-t1,1))+' s')\n",
    "    ##fi\n",
    "    init_n = len(ms)\n",
    "    \n",
    "    # Remove samples with apparent Hmag below faintest APOGEE Hmax\n",
    "    t1 = time.time()\n",
    "    dm = 5.*np.log10(orbs.dist().to(apu.pc).value)-5.\n",
    "    where_good_Hmag1 = np.where(np.nanmax(aposf_Hmax) >\\\n",
    "        Hmag + dm)[0]\n",
    "    orbs = orbs[where_good_Hmag1]\n",
    "    ms = ms[where_good_Hmag1]\n",
    "    dm = dm[where_good_Hmag1]\n",
    "    Hmag = Hmag[where_good_Hmag1]\n",
    "    iso_match_indx = iso_match_indx[where_good_Hmag1]\n",
    "    t2=time.time()\n",
    "    if print_stats:\n",
    "        print(str(len(where_good_Hmag1))+'/'+str(init_n)+\\\n",
    "              ' samples are bright enough to be observed')\n",
    "        print('Kept '+str(round(100*len(where_good_Hmag1)/init_n,2))+\\\n",
    "              ' % of samples')\n",
    "        print('Removing samples with H-band magnitudes below faintest APOGEE'+\\\n",
    "              ' Hmax limit took '+str(round(t2-t1,1))+' s')\n",
    "    ##fi\n",
    "    init_n = len(ms)\n",
    "        \n",
    "    # Remove samples that lie outside the APOGEE observational footprint\n",
    "    t1 = time.time()\n",
    "    fp_indx,locid = _remove_orbits_outside_footprint(orbs,aposf,good_nspec_fields)\n",
    "    orbs = orbs[fp_indx]\n",
    "    ms = ms[fp_indx]\n",
    "    dm = dm[fp_indx]\n",
    "    Hmag = Hmag[fp_indx]\n",
    "    iso_match_indx = iso_match_indx[fp_indx]\n",
    "    t2 = time.time()\n",
    "    if print_stats:        \n",
    "        print('Removing samples outside observational footprint took '+\\\n",
    "              str(round(t2-t1,1))+' s')\n",
    "        print(str(len(fp_indx))+'/'+str(init_n)+' samples found within'+\\\n",
    "              ' observational footprint')\n",
    "        print('Kept '+str(round(100*len(fp_indx)/init_n,2))+\\\n",
    "              ' % of samples')\n",
    "    ##fi\n",
    "    init_n = len(ms)\n",
    "    \n",
    "    # Remove samples with apparent Hmag below faintest Hmax on field-by-field \n",
    "    # basis\n",
    "    t1 = time.time()\n",
    "    field_Hmax = np.nanmax(aposf_Hmax, axis=1)\n",
    "    locid_inds = np.where(locid.reshape(locid.size, 1) == aposf._locations)[1]\n",
    "    Hmax = field_Hmax[locid_inds]\n",
    "    where_good_Hmag2 = np.where(Hmax > Hmag + dm)[0]\n",
    "    # pdb.set_trace()\n",
    "    # Access values with good H-magnitudes\n",
    "    orbs = orbs[where_good_Hmag2]\n",
    "    locid = locid[where_good_Hmag2]\n",
    "    ms = ms[where_good_Hmag2]\n",
    "    dm = dm[where_good_Hmag2]\n",
    "    Hmag = Hmag[where_good_Hmag2]\n",
    "    iso_match_indx = iso_match_indx[where_good_Hmag2]\n",
    "    Jmag = iso['Jmag'][iso_match_indx]\n",
    "    Ksmag = iso['Ksmag'][iso_match_indx]\n",
    "    t2=time.time()\n",
    "    if print_stats:\n",
    "        print('Removing samples with H-band magnitudes outside '+\\\n",
    "              'observational limits took '+str(round(t2-t1,1))+' s')\n",
    "        print(str(len(where_good_Hmag2))+'/'+str(init_n)+\\\n",
    "                  ' samples are bright enough to be observed')\n",
    "        print('Kept '+str(round(100*len(where_good_Hmag2)/init_n,2))+\\\n",
    "              ' % of samples')\n",
    "    ##fi\n",
    "    init_n = len(ms)\n",
    "        \n",
    "    # Get lbIndx for the dust map\n",
    "    gl = orbs.ll(use_physical=True).value\n",
    "    gb = orbs.bb(use_physical=True).value\n",
    "    dist = np.atleast_2d(orbs.dist(use_physical=True).value).T\n",
    "    # Information about the dust map\n",
    "    dmap_nsides = np.array(dmap._nsides)\n",
    "    nside_pix = np.zeros((len(orbs),len(dmap_nsides)))\n",
    "    nside_arr = np.repeat(dmap_nsides[:,np.newaxis],len(orbs),axis=1).T\n",
    "    # Calculate healpix u\n",
    "    dmap_hpu = (dmap._pix_info['healpix_index'] + 4*dmap._pix_info['nside']**2.).astype(int)\n",
    "    hpu = (gh2_nside_pix + 4*gh2_nside_arr**2).astype(int)\n",
    "    # Use searchsorted to get the indices\n",
    "    dmap_hpu_argsort = np.argsort(dmap_hpu)\n",
    "    dmap_hpu_sorted = dmap_hpu[dmap_hpu_argsort]\n",
    "    hpu_indx_sorted = np.searchsorted(dmap_hpu_sorted,hpu)\n",
    "    hpu_indx = np.take(dmap_hpu_argsort, hpu_indx_sorted, mode=\"clip\")\n",
    "    hpu_mask = dmap_hpu[hpu_indx] != hpu\n",
    "    hpu_ma = np.ma.array(hpu_indx, mask=hpu_mask)\n",
    "    lbIndx = hpu_ma.data[~hpu_ma.mask]\n",
    "    t2 = time.time()\n",
    "    if print_stats:\n",
    "        print('Getting lbIndx took '+str(round(t2-t1,1))+' s')\n",
    "    ##fi\n",
    "    \n",
    "    # Compute AH\n",
    "    t1 = time.time()\n",
    "    unique_lbIndx = np.unique(lbIndx).astype(int)\n",
    "    AH = np.zeros(len(orbs))\n",
    "    for i in range(len(unique_lbIndx)):\n",
    "        # First find which samples have this lbIndx\n",
    "        where_unique = np.where(lbIndx == unique_lbIndx[i])[0]\n",
    "        # Get the dust map interpolation data for this lbIndx\n",
    "        dmap_interp_data = scipy.interpolate.InterpolatedUnivariateSpline(\n",
    "            dmap._distmods, dmap._best_fit[unique_lbIndx[i]], k=dmap._interpk)\n",
    "        # Calcualate AH\n",
    "        eBV_to_AH = mwdust.util.extCurves.aebv(dmap._filter,sf10=dmap._sf10)\n",
    "        AH[where_unique] = dmap_interp_data(dm[where_unique])*eBV_to_AH\n",
    "    ###i\n",
    "    t2 = time.time()\n",
    "#     if print_stats:\n",
    "#         print('Getting AH took '+str(t2-t1)+' s')\n",
    "#     ##fi\n",
    "    \n",
    "    # Apply the selection function\n",
    "    t1 = time.time()\n",
    "    sf_keep_indx = np.zeros(len(orbs),dtype=bool)\n",
    "    for i in range(len(orbs)):\n",
    "        random_n = np.random.random(size=1)[0]\n",
    "        _H = Hmag[i] + dm[i] + AH[i]\n",
    "        _JK0 = Jmag[i] - Ksmag[i]\n",
    "        compare_n = aposf(locid[i],_H,_JK0)\n",
    "        if compare_n > random_n: sf_keep_indx[i] = True\n",
    "    t2 = time.time()\n",
    "    if print_stats:\n",
    "        print('Applying selection function took '+str(round(t2-t1,1))+' s')\n",
    "        print(str(np.sum(sf_keep_indx))+'/'+str(init_n)+\\\n",
    "                  ' samples survive the selection function')\n",
    "        print('Kept '+str(round(100*np.sum(sf_keep_indx)/init_n,2))+\\\n",
    "              ' % of samples')\n",
    "    ##fi\n",
    "    \n",
    "    orbs = orbs[sf_keep_indx]\n",
    "    locid = locid[sf_keep_indx]\n",
    "    ms = ms[sf_keep_indx]\n",
    "    iso_match_indx = iso_match_indx[sf_keep_indx]\n",
    "    \n",
    "    return orbs, locid, ms, iso_match_indx\n",
    "#def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load selection function, dust map, isochrone, mock orbits and masses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aposf_data_dir = '/geir_data/scr/lane/projects/ges-mass/data/gaia_apogee/'+\\\n",
    "                 'apogee_dr16_l33_gaia_dr2/'\n",
    "with open(aposf_data_dir+'apogee_SF.dat','rb') as f:\n",
    "    aposf = pickle.load(f)\n",
    "##wi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmap = mwdust.Combined19(filter='2MASS H') # dustmap from mwdust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 0.0010\n",
    "log_age = 10.0\n",
    "iso = load_parsec_isochrone(iso_dir=os.environ['ISODIST_DATA'],\n",
    "                           z=z,log_age=log_age, remove_wd_point=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_dir = '/geir_data/scr/lane/projects/ges-mass/apo_mocks/'\n",
    "n_samples_str = '1e7'\n",
    "with open(sample_data_dir+'orbs_'+n_samples_str+'.pkl','rb') as f:\n",
    "    orbs = pickle.load(f)\n",
    "##fi\n",
    "ms = np.load(sample_data_dir+'masses_'+n_samples_str+'.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(iso['Jmag']-iso['Ksmag'], iso['Hmag'], c=np.arange(0,len(iso),1.))\n",
    "ax.invert_yaxis()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove any fields without spectroscopic targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nspec = np.nansum(aposf._nspec_short,axis=1) +\\\n",
    "        np.nansum(aposf._nspec_medium,axis=1) +\\\n",
    "        np.nansum(aposf._nspec_long,axis=1)\n",
    "good_nspec_fields = np.where(nspec>=1.)[0]\n",
    "print(str(len(good_nspec_fields))+'/'+str(len(nspec))+' fields have'\\\n",
    "      ' spectroscopic targets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here do isochrone matching first, and a coarse removal of samples, then field matching\n",
    "Match into the isochrone first, then remove all samples with apparent Hmag\n",
    "below the faintest observational limit in APOGEE (haven't matched to fields yet)\n",
    "so can't use field-specific values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match to isochrone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "# Match samples to isochrone entries based on initial mass\n",
    "m_err = 1e-4+np.diff(iso['Mini']).max()/2.\n",
    "good_iso_match,iso_match_indx = _match_isochrone_to_samples(iso,ms,m_err=m_err)\n",
    "print(str(len(good_iso_match))+'/'+str(len(ms))+\\\n",
    "      ' samples have good matches in the isochrone')\n",
    "print('Kept '+str(round(100*len(good_iso_match)/len(ms),2))+' % of samples')\n",
    "assert np.all(np.abs(ms[good_iso_match]-iso['Mini'][iso_match_indx]<=m_err))\n",
    "\n",
    "# Access isochrone-matched values\n",
    "im_orbs = orbs[good_iso_match]\n",
    "im_ms = ms[good_iso_match]\n",
    "im_Hmag = iso['Hmag'][iso_match_indx]\n",
    "\n",
    "t2 = time.time()\n",
    "print('Matching samples to isochrone entries took '+str(round(t2-t1,1))+' s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(iso['Jmag'][iso_match_indx][::100]\\\n",
    "           -iso['Ksmag'][iso_match_indx][::100], \n",
    "           im_Hmag[::100], alpha=0.1, s=1.)\n",
    "ax.set_ylabel('H')\n",
    "ax.set_xlabel('J-Ks')\n",
    "ax.invert_yaxis()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove samples that lie below the faintest APOGEE Hmax\n",
    "Should be about 0.1 per cent depending on the isochrone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "# Find maximum Hmag values for all cohorts in each field and compare to the \n",
    "# distance modulus-adjusted value for each sample\n",
    "aposf_Hmax = np.nanmax(np.dstack([aposf._short_hmax,\n",
    "                                  aposf._medium_hmax,\n",
    "                                  aposf._long_hmax])[0])\n",
    "im_dm = 5.*np.log10(im_orbs.dist().to(apu.pc).value)-5.\n",
    "where_good_Hmag = np.where(aposf_Hmax > (im_Hmag+im_dm) )[0]\n",
    "where_bad_Hmag = np.where(aposf_Hmax < (im_Hmag+im_dm) )[0]\n",
    "print(str(len(where_good_Hmag))+'/'+str(len(im_Hmag))+\\\n",
    "          ' samples are bright enough to be observed')\n",
    "print('Kept '+str(round(100*len(where_good_Hmag)/len(im_Hmag),2))+' % of samples')\n",
    "\n",
    "# Access values with good H-magnitudes\n",
    "gh_orbs = im_orbs[where_good_Hmag]\n",
    "gh_ms = im_ms[where_good_Hmag]\n",
    "gh_dm = im_dm[where_good_Hmag]\n",
    "gh_Hmag = im_Hmag[where_good_Hmag]\n",
    "gh_iso_match_indx = iso_match_indx[where_good_Hmag]\n",
    "\n",
    "t2=time.time()\n",
    "print('Removing samples with H-band magnitudes outside observational limits'+\\\n",
    "      ' took '+str(round(t2-t1,1))+' s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(iso['Jmag'][iso_match_indx[where_bad_Hmag]][::100]\\\n",
    "           -iso['Ksmag'][iso_match_indx[where_bad_Hmag]][::100], \n",
    "           im_Hmag[where_bad_Hmag][::100]+im_dm[where_bad_Hmag][::100], \n",
    "           alpha=0.1, s=1.)\n",
    "ax.axhline(aposf_Hmax, c='Black', linestyle='solid')\n",
    "    \n",
    "ax.set_ylabel('H')\n",
    "ax.set_xlabel('J-Ks')\n",
    "ax.invert_yaxis()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(iso['Jmag'][gh_iso_match_indx]-iso['Ksmag'][gh_iso_match_indx], \n",
    "           gh_Hmag+gh_dm, alpha=0.1, s=1.)\n",
    "ax.axhline(aposf_Hmax, c='Black', linestyle='solid')\n",
    "    \n",
    "ax.set_ylabel('H')\n",
    "ax.set_xlabel('J-Ks')\n",
    "ax.invert_yaxis()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plot_gl = gh_orbs.ll(use_physical=True).value\n",
    "plot_gb = gh_orbs.bb(use_physical=True).value\n",
    "\n",
    "plot_gl[plot_gl > 180] = plot_gl[plot_gl > 180] - 360\n",
    "\n",
    "ax.scatter(plot_gl,plot_gb,color='Black',alpha=0.5,s=0.1)\n",
    "ax.set_xlim(180,-180)\n",
    "ax.set_ylim(-90,90)\n",
    "ax.set_xlabel(r'$\\ell$')\n",
    "ax.set_ylabel(r'$b$')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove orbits outside APOGEE fields\n",
    "Should keep ~12% of orbits depending on APOGEE DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "fp_indx,fp_locid = _remove_orbits_outside_footprint(gh_orbs,aposf,good_nspec_fields)\n",
    "fp_orbs = gh_orbs[fp_indx]\n",
    "fp_ms = gh_ms[fp_indx]\n",
    "fp_Hmag = gh_Hmag[fp_indx]\n",
    "fp_dm = gh_dm[fp_indx]\n",
    "fp_iso_match_indx = gh_iso_match_indx[fp_indx]\n",
    "t2 = time.time()\n",
    "print('Removing samples outside observational footprint took '+\\\n",
    "      str(round(t2-t1,1))+' s')\n",
    "print(str(len(fp_indx))+'/'+str(len(gh_orbs))+' samples found within observational footprint')\n",
    "print('Kept '+str(round(100*len(fp_indx)/len(gh_orbs),2))+' % of samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plot_gl = fp_orbs.ll(use_physical=True).value\n",
    "plot_gb = fp_orbs.bb(use_physical=True).value\n",
    "\n",
    "plot_gl[plot_gl > 180] = plot_gl[plot_gl > 180] - 360\n",
    "\n",
    "ax.scatter(plot_gl,plot_gb,color='Black',alpha=0.5,s=0.1)\n",
    "ax.set_xlim(180,-180)\n",
    "ax.set_ylim(-90,90)\n",
    "ax.set_xlabel(r'$\\ell$')\n",
    "ax.set_ylabel(r'$b$')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do a more specific field-by-field Hmax removal procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "# Find maximum Hmag values for all cohorts in each field and compare to the \n",
    "# distance modulus-adjusted value for each sample\n",
    "field_Hmax = np.nanmax(np.dstack([aposf._short_hmax,\n",
    "                                  aposf._medium_hmax,\n",
    "                                  aposf._long_hmax])[0],axis=1)\n",
    "fp_locid_inds = np.where(fp_locid.reshape(fp_locid.size, 1) == aposf._locations)[1]\n",
    "fp_Hmax = field_Hmax[fp_locid_inds]\n",
    "where_good_Hmag = np.where(fp_Hmax > fp_Hmag + fp_dm)[0]\n",
    "# Access values with good H-magnitudes\n",
    "gh2_orbs = fp_orbs[where_good_Hmag]\n",
    "gh2_locid = fp_locid[where_good_Hmag]\n",
    "gh2_ms = fp_ms[where_good_Hmag]\n",
    "gh2_dm = fp_dm[where_good_Hmag]\n",
    "gh2_Hmag = fp_Hmag[where_good_Hmag]\n",
    "gh2_iso_match_indx = fp_iso_match_indx[where_good_Hmag]\n",
    "gh2_Jmag = iso['Jmag'][gh2_iso_match_indx]\n",
    "gh2_Ksmag = iso['Ksmag'][gh2_iso_match_indx]\n",
    "\n",
    "t2=time.time()\n",
    "print('Removing samples with H-band magnitudes outside observational limits'+\\\n",
    "      ' took '+str(round(t2-t1,1))+' s')\n",
    "print(str(len(where_good_Hmag))+'/'+str(len(fp_Hmag))+\\\n",
    "          ' samples are bright enough to be observed')\n",
    "print('Kept '+str(round(100*len(where_good_Hmag)/len(fp_Hmag),2))+\\\n",
    "      ' % of samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "field_Hmax[np.isnan(field_Hmax)] = 9999\n",
    "ax.hist(field_Hmax[field_Hmax < 9998], histtype='step')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(gh2_Jmag-gh2_Ksmag,gh2_Hmag+gh2_dm,alpha=1.,s=1.,\n",
    "           color='Black')\n",
    "ax.set_xlabel(r'$(J-K_{s})_{0}$')\n",
    "ax.set_ylabel(r'$H_{app}$')\n",
    "ax.set_xlim(0.3,1.0)\n",
    "# ax.set_ylim(5.,15.)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plot_gl = gh2_orbs.ll(use_physical=True).value\n",
    "plot_gb = gh2_orbs.bb(use_physical=True).value\n",
    "\n",
    "plot_gl[plot_gl > 180] = plot_gl[plot_gl > 180] - 360\n",
    "\n",
    "ax.scatter(plot_gl,plot_gb,color='Black',alpha=0.5,s=1.)\n",
    "ax.set_xlim(180,-180)\n",
    "ax.set_ylim(-90,90)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use the dustmap to get the redenning correction\n",
    "Looping through calls to dust map is slow. Perhaps minor speedup by first \n",
    "determining lbIndx and then batching calls to the dust map for samples with the \n",
    "same lbIndx but different distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get l,b,d properties for orbits\n",
    "gh2_gl = gh2_orbs.ll(use_physical=True).value\n",
    "gh2_gb = gh2_orbs.bb(use_physical=True).value\n",
    "gh2_dist = np.atleast_2d(gh2_orbs.dist(use_physical=True).value).T\n",
    "\n",
    "# Prepare arrays that hold healpix info about samples\n",
    "dmap_nsides = np.array(dmap._nsides)\n",
    "gh2_nside_pix = np.zeros((len(gh2_orbs),len(dmap_nsides)))\n",
    "gh2_nside_arr = np.repeat(dmap_nsides[:,np.newaxis],len(gh2_orbs),axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "gh2_AH_slow = np.zeros(len(gh2_orbs))\n",
    "for i in range(len(gh2_AH_slow)):\n",
    "    print('Done '+str(i+1)+'/'+str(len(gh2_AH_slow)),end='\\r')\n",
    "    gh2_AH_slow[i] = dmap(gh2_gl[i],gh2_gb[i],gh2_dist[i])\n",
    "###i\n",
    "t2 = time.time()\n",
    "print('Getting AH with dmap queries took '+str(t2-t1)+' s')\n",
    "\n",
    "t1 = time.time()\n",
    "gh2_lbIndx_slow = np.zeros(len(gh2_orbs))\n",
    "for i in range(len(gh2_orbs)):\n",
    "    print('Done '+str(i+1)+'/'+str(len(gh2_lbIndx_slow)),end='\\r')\n",
    "    gh2_lbIndx_slow[i] = dmap._lbIndx(gh2_gl[i],gh2_gb[i])\n",
    "###i\n",
    "t2 = time.time()\n",
    "print('Getting lbIndx took '+str(t2-t1)+' s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when querying the dust map the most expensive operation is determining the lbIndx.\n",
    "Figure out a way to do this faster, preferably vectorizable\n",
    "\n",
    "### Faster method of getting $A_{H}$\n",
    "\n",
    "Querying the healpix pixel can be fast if we use `searchsorted` to get matches \n",
    "in the unique healpix identifier:\n",
    "\n",
    "$u = p + 4N_{side}^{2}$\n",
    "\n",
    "where $p$ is the healpix index in the range\n",
    "\n",
    "$p \\in [0,12N_{side}^{2}-1]$\n",
    "\n",
    "Calculate this for the healpix tiles in the dust map as well as the \n",
    "samples. Then querying is easy.\n",
    "\n",
    "Then bulk evaluate the dust map for all samples that share an lbIndx but have different distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "_DEGTORAD = np.pi/180.\n",
    "for i in range(len(dmap_nsides)):\n",
    "    gh2_nside_pix[:,i] = healpy.pixelfunc.ang2pix(dmap_nsides[i],\n",
    "                                                 (90.-gh2_gb)*_DEGTORAD,\n",
    "                                                  gh2_gl*_DEGTORAD,\n",
    "                                                  nest=True)\n",
    "\n",
    "# Calculate u for both the dust map and the samples\n",
    "dmap_hpu = (dmap._pix_info['healpix_index'] + 4*dmap._pix_info['nside']**2.).astype(int)\n",
    "gh2_hpu = (gh2_nside_pix + 4*gh2_nside_arr**2).astype(int)\n",
    "\n",
    "# Use searchsorted to get the indices\n",
    "dmap_hpu_argsort = np.argsort(dmap_hpu)\n",
    "dmap_hpu_sorted = dmap_hpu[dmap_hpu_argsort]\n",
    "hpu_indx_sorted = np.searchsorted(dmap_hpu_sorted,gh2_hpu)\n",
    "hpu_indx = np.take(dmap_hpu_argsort, hpu_indx_sorted, mode=\"clip\")\n",
    "hpu_mask = dmap_hpu[hpu_indx] != gh2_hpu\n",
    "hpu_ma = np.ma.array(hpu_indx, mask=hpu_mask)\n",
    "gh2_lbIndx = hpu_ma.data[~hpu_ma.mask]\n",
    "\n",
    "t2 = time.time()\n",
    "print('Getting lbIndx using searchsorted took '+str(round(t2-t1,1))+' s')\n",
    "\n",
    "# Sanity checks, there should only be one matching Nside per sample so if \n",
    "# we collapse the array there should be exactly enough entries. lbIndx acquired\n",
    "# using this method should also match lbIndx acquired using the slow method.\n",
    "assert np.all( np.sum(~hpu_ma.mask,axis=1) == np.ones(len(gh2_orbs)) )\n",
    "assert np.all( gh2_lbIndx == gh2_lbIndx_slow )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "# First get all the unique lbIndx values, then batch compute AH\n",
    "unique_lbIndx = np.unique(gh2_lbIndx).astype(int)\n",
    "gh2_AH = np.zeros(len(gh2_orbs))\n",
    "for i in range(len(unique_lbIndx)):\n",
    "    # First find which samples have this lbIndx\n",
    "    where_unique = np.where(gh2_lbIndx == unique_lbIndx[i])[0]\n",
    "    # Get the dust map interpolation data for this lbIndx\n",
    "    dmap_interp_data = scipy.interpolate.InterpolatedUnivariateSpline(\n",
    "        dmap._distmods, dmap._best_fit[unique_lbIndx[i]], k=dmap._interpk)\n",
    "    # Calcualate AH\n",
    "    eBV_to_AH = mwdust.util.extCurves.aebv(dmap._filter,sf10=dmap._sf10)\n",
    "    gh2_AH[where_unique] = dmap_interp_data(gh2_dm[where_unique])*eBV_to_AH\n",
    "###i\n",
    "\n",
    "t2 = time.time()\n",
    "print('Getting AH took '+str(t2-t1)+' s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(gh2_AH - gh2_AH_slow < 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.hist(gh2_AH,histtype='step',edgecolor='Black')\n",
    "\n",
    "ax.set_xlabel(r'$A_{H}$')\n",
    "ax.set_ylabel('N')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(gh2_Jmag-gh2_Ksmag,gh2_Hmag+gh2_dm+gh2_AH,alpha=1.,s=1.,\n",
    "           color='Black')\n",
    "ax.set_xlabel(r'$(J-K_{s})_{0}$')\n",
    "ax.set_ylabel(r'$H_{app}$')\n",
    "ax.set_xlim(0.3,1.0)\n",
    "# ax.set_ylim(5.,20.)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the selection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "sf_keep_indx = np.zeros(len(gh2_orbs),dtype=bool)\n",
    "for i in range(len(gh2_orbs)):\n",
    "    random_n = np.random.random(size=1)[0]\n",
    "    _H = gh2_Hmag[i] + gh2_dm[i] + gh2_AH[i]\n",
    "    _JK0 = gh2_Jmag[i] - gh2_Ksmag[i]\n",
    "    compare_n = aposf(gh2_locid[i],_H,_JK0)\n",
    "    # print(str(compare_n)+' > '+str(random_n))\n",
    "    if compare_n > random_n: sf_keep_indx[i] = True\n",
    "t2 = time.time()\n",
    "print('Applying selection function took '+str(round(t2-t1,1))+' s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mass of the profile is: '+str(np.sum(ms)/1e8)+'e8 Msun')\n",
    "print('Number of surviving stars: '+str(np.sum(sf_keep_indx)))"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
